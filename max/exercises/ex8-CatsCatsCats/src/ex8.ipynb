{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d182baf9",
   "metadata": {},
   "source": [
    "# Exercise 8 - Cats, Cats, and EigenCats \n",
    "\n",
    "Are you sad that you have watched all cat movies and seen all cat photos on the internet? Then be sad no more - in this exercise we will make a *Cat Synthesizer* where you can create all the cat photos you will ever need!\n",
    "\n",
    "Secondly, a very unfortunate event happened so you are now in a situation, where you need to find a *perfect new twin cat*.\n",
    " \n",
    "To be able to do these wonderful things we will harness the power of image based *principal component analysis*. The methods we will use, can be called *classical machine learning*.\n",
    "\n",
    "![Missing Cat](figures/MissingCatProcessed.jpg)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this exercise, the student should be able to do the following:\n",
    "\n",
    "1. Preprocess a batch of images so they can be used to machine learning. Preprocessing can include geometric transformations, intensity transformations and image cropping. \n",
    "2. Use the python function `glob` to find all files with a given pattern in a folder.\n",
    "3. Create an empty data matrix that can hold a given set of images and a given number of measurements per image.\n",
    "4. Compute the number of features per image using the height, width and the number of channels in the image.\n",
    "5. Use the function `flatten` to convert an image into a 1-D vector.\n",
    "6. Create an image from a 1-D vector by using the [`reshape`](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) function.\n",
    "7. Create an unsigned byte image from a float image using pixel value scaling and pixel type conversion.\n",
    "8. Read a set of images and put their pixel values into a data matrix.\n",
    "9. Compute an average image using the data matrix.\n",
    "10. Visualize an average image\n",
    "11. Preprocess one image so it can be used in machine learning.\n",
    "12. Use sum-of-squared pixel differences (SSD) to compare one image with all images in a training set.\n",
    "13. Identify and visualize the images in the training set with the smallest and largest SSD compared to a given image.\n",
    "14. Do a principal component analysis (PCA) of the data matrix using the [sci-kit learn PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "15. Select the number of components that should be computed in the PCA.\n",
    "16. Extract and visualize the amount of the total variation that each principal component explains. \n",
    "17. Project the data matrix into PCA space.\n",
    "18. Plot the first two dimensions of the PCA space. The PCA space is the positions of each sample when projected onto the principal components.\n",
    "19. Identify and visualize the images that have extreme positions in PCA. For example the images that have the largest and smallest coordinates in PCA space.\n",
    "20. Compute and visualize a synthetic image by adding linear combinations of principal components to the average image.\n",
    "21. Select suitable weights based on the PCA space for synthesizing images.\n",
    "22. Compute and visualize the major modes of variation in the training set, by sampling along the principal components.\n",
    "23. Generate random synthetic images that lies within the PCA space of the training set.\n",
    "24. Project a given image into PCA space\n",
    "25. Generate a synthetich version of an image by using the image position in PCA space.\n",
    "26. Compute the Euclidean distance in PCA space between a given image and all other images.\n",
    "27. Identify and visualize the images in the training set with the smallest and largest Euclidean distance in PCA space to a given image.\n",
    "28. Use [`argpartition`](https://numpy.org/doc/stable/reference/generated/numpy.argpartition.html) to find the N closest images in PCA space to a given image.\n",
    "\n",
    "## Importing required Python packages\n",
    "\n",
    "We will use the virtual environment from the previous exercise (`course02503`). \n",
    "\n",
    "Let us start with some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014eff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from skimage.util import img_as_ubyte\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.transform import SimilarityTransform\n",
    "from skimage.transform import warp\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eccd28",
   "metadata": {},
   "source": [
    "## Exercise data and material\n",
    "\n",
    "The data and material needed for this exercise can be found here: \n",
    "[exercise data and material](https://github.com/RasmusRPaulsen/DTUImageAnalysis/tree/main/exercises/ex8-CatsCatsCats/data)\n",
    "\n",
    "The main part of the data is a large database of photos of cats, where there are also a set of landmarks per photo. You should download the data [here](https://courses.compute.dtu.dk/02502/data/training_cats.zip). \n",
    "\n",
    "**IMPORTANT:** You can start by working with the smaller data set with 100 cats found here: [smaller photo database of 100 cats](https://courses.compute.dtu.dk/02502/data/training_cats_100.zip). Later you can use the full data set and see if your computer has enough RAM to handle it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f3014",
   "metadata": {},
   "source": [
    "Start by unpacking all the training photos in folder you choose. For example `training_data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c46c11",
   "metadata": {},
   "source": [
    "## Preprocessing data for machine learning\n",
    "\n",
    "The photos contains cats in many situations and backgrounds. To make it easier to do machine learning, we will *preprocess* the data, so the photos only contains the face of the cat. Preprocessing is and important step in most machine learning approaches.\n",
    "\n",
    "The preprocessing steps are:\n",
    "\n",
    "- Define a model cat (`ModelCat.jpg`) with associated landmarks (`ModelCat.jpg.cat`)\n",
    "- For each cat in the training data:\n",
    "  - Use landmark based registration with a *similarity transform* to register the photo to the model cat\n",
    "  - Crop the registered photo\n",
    "  - Save the result in a fold called **preprocessed**\n",
    "\n",
    "**Exercise 1:** *Preprocess all image in the training set. To do the preprocessing, you can use the code snippets supplied* [here](https://github.com/RasmusRPaulsen/DTUImageAnalysis/tree/main/exercises/ex8-CatsCatsCats/data). There is also a **Model Cat** supplied.\n",
    "\n",
    "The result of the preprocessing is a directory containing smaller photos containing cat faces. All the preprocessed photos also have the same size. \n",
    "\n",
    "## Gathering data into a data matrix\n",
    "\n",
    "To start, we want to collect all the image data into a data matrix. The matrix should have dimensions `[n_samples, n_features]` where **n_samples** is the number of photos in our training set and **n_features** is the number of values per image. Since we are working with RGB images, the number of features are given by `n_features = height * width * channels`, where `channels = 3`. \n",
    "\n",
    "The data matrix can be constructed by:\n",
    "\n",
    "- Find the number of image files in the **preprocessed** folder using [`glob`](https://docs.python.org/3/library/glob.html). Look at the `preprocess_all_cats` function to get an idea of how to use `glob`. \n",
    "- Read the first photo and use that to find the height and width of the photos\n",
    "- Set **n_samples** and **n_features**\n",
    "- Make an empty matrix `data_matrix = np.zeros((n_samples, n_features))`\n",
    "- Read the image files one by one and use `flatten()` to make each image into a 1-D vector (flat_img). \n",
    "- Put the image vector (flat_img) into the data matrix by for example `data_matrix[idx, :] = flat_img` , where idx is the index of the current image.\n",
    "\n",
    "**Exercise 2:** *Compute the data matrix.* \n",
    "\n",
    "## Compute and visualize a mean cat\n",
    "\n",
    "In the data matrix, one row is one cat. You can therefore compute an average cat, **The Mean Cat** by computing one row, where each element is the average of the column. \n",
    "\n",
    "**Exercise 3:** *Compute the average cat.* \n",
    "\n",
    "You can use the supplied function `create_u_byte_image_from_vector` to create an image from a 1-D image vector.\n",
    "\n",
    "**Exercise 4:** *Visualize the Mean Cat* \n",
    "\n",
    "## Find a missing cat or a cat that looks like it (using image comparison)\n",
    "\n",
    "You have promised to take care of your neighbours cat while they are on vacation. But...Oh! no! You were in such a hurry to get to DTU that you forgot to close a window. Now the cat is gone!!! What to do? \n",
    "\n",
    "**Exercise 5:** *Decide that you quickly buy a new cat that looks very much like the missing cat - so nobody notices*\n",
    "\n",
    "Luckily, the training set is actually photos of cats that are in a *get a new cat cheap* nearby store. \n",
    "\n",
    "To find a cat that looks like the missing cat, you start by comparing the missing cat pixels to the pixels of the cats in the training set. The comparison between the missing cat data and the training data can be done using the sum-of-squared differences (SSD).\n",
    "\n",
    "**Exercise 6:** *Use the `preprocess_one_cat` function to preprocess the photo of the poor missing cat*\n",
    "\n",
    "**Exercise 7:** *Flatten the pixel values of the missing cat so it becomes a vector of values.*\n",
    "\n",
    "**Exercise 8:** *Subtract you missing cat data from all the rows in the data_matrix and for each row compute the sum of squared differences. This can for example be done by:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9dcc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = data_matrix - im_miss_flat\n",
    "sub_distances = np.linalg.norm(sub_data, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019c2e1",
   "metadata": {},
   "source": [
    "**Exercise 9:** *Find the cat that looks most like your missing cat by finding the cat, where the SSD is smallest. You can for example use `np.argmin`.*\n",
    "\n",
    "**Exercise 10:** *Extract the found cat from the data_matrix and use `create_u_byte_image_from_vector` to create an image that can be visualized. Did you find a good replacement cat? Do you think your neighbour will notice? Even with their glasses on?*\n",
    "\n",
    "**Exercise 11:** *You can use `np.argmax` to find the cat that looks the least like the missing cat.*\n",
    "\n",
    "You can also use your own photo of a cat (perhaps even your own cat). To do that you should:\n",
    "\n",
    "- Place a jpg version of your cat photo in the folder where you had your missing cat photo. Call it for example **mycat.jpg**\n",
    "- Create a landmark file called something like **mycat.jpg.cat**. It is a text file.\n",
    "- In the landmark file you should create three landmarks: `3 189 98 235 101 213 142` . Here the first `3` just say there are three landmarks. The following 6 numbers are the (x, y) positions of the right eye, the left eye and the nose. You should manually update these numbers.\n",
    "- Use the `preprocess_one_cat` function to preprocess the photo\n",
    "- Now you can do the above routine to match your own cat.\n",
    "\n",
    "**Optional Exercise:** *Use a photo of your own cat to find its twins*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598909d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Principal component analysis on the cats \n",
    "\n",
    "We now move to more classical machine learning on cats. Namely Principal component analysis  (PCA) analysis of the cats image.\n",
    "\n",
    "To compute the PCA, we use the [sci-kit learn PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Note that this version of PCA automatically *centers* data. It means that it will subtract the average cat from all cats for you.\n",
    "\n",
    "**Exercise 12:** *Start by computing the first 50 principal components:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing PCA\")\n",
    "cats_pca = PCA(n_components=50)\n",
    "cats_pca.fit(data_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b8b6dd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "This might take some time. If your compute can not handle so many images, you can manually move or delete som photos out of the **preprocessed** folder before computing the data matrix.\n",
    "\n",
    "The amount of the total variation that each component explains can be found in `cats_pca.explained_variance_ratio_`.\n",
    "\n",
    "**Exercise 13:** *Plot the amount of the total variation explained by each component as function of the component number.*\n",
    "\n",
    "**Exercise 14:** *How much of the total variation is explained by the first component?*\n",
    "\n",
    "We can now project all out cat images into the PCA space (that is 50 dimensional):\n",
    "\n",
    "**Exercise 15:** *Project the cat images into PCA space*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = cats_pca.transform(data_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6465abd1",
   "metadata": {},
   "source": [
    "Now each cat has a position in PCA space. For each cat this position is 50-dimensional vector. Each value in this vector describes *how much of that component* is present in that cat photo.\n",
    "\n",
    "We can plot the first two dimensions of the PCA space, to see where the cats are placed. The first PCA coordinate for all the cats can be found using `pc_1 = components[:, 0]` .\n",
    "\n",
    "**Exercise 16:** *Plot the PCA space by plotting all the cats first and second PCA coordinates in a (x, y) plot*\n",
    "\n",
    "## Cats in space\n",
    "\n",
    "We would like to explore what the PCA learnt about our cats in the data set. \n",
    "\n",
    "### Extreme cats\n",
    "\n",
    "We start by finding out which cats that have the most *extreme coordinates* in PCA space. \n",
    "\n",
    "**Exercise 17:** *Use `np.argmin` and `np.argmax` to find the ids of the cats that have extreme values in the first and second PCA coordinates. Extract the cats data from the data matrix and use `create_u_byte_image_from_vector` to visualize these cats. Also plot the PCA space where you plot the extreme cats with another marker and color.*\n",
    "\n",
    "**Exercise 18:** *How do these extreme cat photo look like? Are some actually of such bad quality that they should be removed from the training set? If you remove images from the training set, then you should run the PCA again. Do this until you are satisfied with the quality of the training data.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ce4b7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### The first synthesized cat\n",
    "\n",
    "We can use the PCA to make a so-called **generative model** that can create synthetic samples from the learnt data. It is done by adding a linear combination of principal components to the average cat image:\n",
    "\n",
    "$$\n",
    "I_\\text{synth} = I_\\text{average} + w_1 * P_1 + w_2 * P_2 + \\ldots + w_k * P_k \\enspace ,\n",
    "$$\n",
    "\n",
    "where we $P_1$ is the first principal component, $P_2$ the second and so on. Here we use $k$ principal components.\n",
    "\n",
    "The principal components are stored in `cats_pca.components_`. So the first principal component is `cats_pca.components_[0, :]` .\n",
    "\n",
    "**Exercise 19:** *Create your first fake cat using the average image and the first principal component. You should choose experiment with different weight values (w)* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_cat = average_cat + w * cats_pca.components_[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365066cf",
   "metadata": {},
   "source": [
    "**Exercise 20:** *Use `create_u_byte_image_from_vector` visualize your fake cat.*\n",
    "\n",
    "You can use the PCA plot we did before to select some suitable values for w.\n",
    "\n",
    "**Exercise 21:** *Synthesize some cats, where you use both the first and second principal components and select their individual weights based on the PCA plot.*\n",
    "\n",
    "### The major cat variation in the data set\n",
    "\n",
    "A very useful method to get an overview of the **major modes of variation** in a dataset is to synthesize the samples that are lying on the outer edges of the PCA space.\n",
    "\n",
    "If we for example move a distance out of the first principal axis we can synthesize the cat image there. In this case we will try to move to $\\pm \\sqrt(\\text{explained variance})$, where *explained variance* is the variance explained by that principal component. In code, this will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c57c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_cat_plus = average_cat + 3 * np.sqrt(cats_pca.explained_variance_[m]) * cats_pca.components_[m, :]\n",
    "synth_cat_minus = average_cat - 3 * np.sqrt(cats_pca.explained_variance_[m]) * cats_pca.components_[m, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee35cc5",
   "metadata": {},
   "source": [
    "here **m** is the principal component that we are investigating.\n",
    "\n",
    "**Exercise 22:** *Synthesize and visualize cats that demonstrate the first three major modes of variation. Try show the average cat in the middle of a plot, with the negative sample to the left and the positive to the right. Can you recognise some visual patterns in these modes of variation?*\n",
    "\n",
    "### The Cat Synthesizer (EigenCats)\n",
    "\n",
    "We are now ready to make true cat synthesizer, where cat images are synthesized based on random locations in PCA space. You can start by setting your `synth_cat = average_cat`. Then you can add all the components you want by for example (this number should be less or equal to the number of components we asked the PCA to compute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ada1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_to_use = 10\n",
    "synth_cat = average_cat\n",
    "for idx in range(n_components_to_use):\n",
    "\tw = random.uniform(-1, 1) * 3 * np.sqrt(cats_pca.explained_variance_[idx])\n",
    "\tsynth_cat = synth_cat + w * cats_pca.components_[idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e49df5",
   "metadata": {},
   "source": [
    "**Exercise 23:** *Generate as many cat photos as your heart desires.*.\n",
    "\n",
    "## Cat identification in PCA space\n",
    "\n",
    "Now back to your missing cat. We could find similar cats by computing the difference between the missing cat and all the photos in the databased. Imagine that you only needed to store the 50 weights per cats in your database to do the same type of identification?\n",
    "\n",
    "**Exercise 24:** *Start by finding the PCA space coordinates of your missing cat:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_miss = io.imread(\"data/cats/MissingCatProcessed.jpg\")\n",
    "im_miss_flat = im_miss.flatten()\n",
    "im_miss_flat = im_miss_flat.reshape(1, -1)\n",
    "pca_coords = cats_pca.transform(im_miss_flat)\n",
    "pca_coords = pca_coords.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4679cd7c",
   "metadata": {},
   "source": [
    "The `flatten` calls are needed to bring the arrays into the right shapes.\n",
    "\n",
    "**Exercise 25:** *Plot all the cats in PCA space using the first two dimensions. Plot your missing cat in the same plot, with another color and marker. Is it placed somewhere sensible and does it have close neighbours?*\n",
    "\n",
    "We can generate the synthetic cat that is the closest to your missing cat, by using the missing cats position in PCA space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb88ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_to_use = ?\n",
    "synth_cat = average_cat\n",
    "for idx in range(n_components_to_use):\n",
    "\tsynth_cat = synth_cat + pca_coords[idx] * cats_pca.components_[idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0181b8",
   "metadata": {},
   "source": [
    "**Exercise 26:** *Generate synthetic versions of your cat, where you change the n_components_to_use from 1 to for example 50.*\n",
    "\n",
    "We can compute Euclidean distances in PCA space between your cat and all the other cats by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d018b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sub = components - pca_coords\n",
    "pca_distances = np.linalg.norm(comp_sub, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e00420",
   "metadata": {},
   "source": [
    "**Exercise 27:** *Find the id of the cat that has the smallest and largest distance in PCA space to your missing cat. Visualize these cats. Are they as you expected? Do you think your neighours will notice a difference?*\n",
    "\n",
    "You can also find the n closest cats by using the `np.argpartition` function. \n",
    "\n",
    "**Exercise 28:** *Find the ids of and visualize the 5 closest cats in PCA space. Do they look like your cat?*\n",
    "\n",
    "What we have been doing here is what has been used for face identification and face recognition (*Matthew Turk and Alex Pentland: Eigenfaces for Recognition, 1991*)\n",
    "\n",
    "In summary, we can now synthesize all the cat photos you will only need and we can help people that are loooking for cats that looks like another cat. On top of that, we can now use methods that are considered state-of-the-art before the step into deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bbbc9d",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Cat data set](https://www.kaggle.com/datasets/crawford/cat-dataset)\n",
    "- [sci-kit learn PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
